# sc_cluster
Single cell clustering experiments that use **AIDE** to reduce the dimension of cell-gene matrix and then use **RPH-KMeans** to do clustering.

## Installation
- [AIDE](https://github.com/tinglabs/aide): Autoencoder-imputed distance-preserved embedding, which combines both **Multidimentional Scaling (MDS)** and **AutoEncoder (AE)** technique, aiming to preserve the distance between imputed data generated by **AE** when reducing dimension.
- [rph-kmeans](https://github.com/tinglabs/rph_kmeans): a variant of kmeans algorithm in which the initial centers is produced by point reduction process using **random projection (RP)**, which is one of the **local sensitive hashing (LSH)** technique.


## Single Cell Clustering Demo
### For Small Data
A demo is provided in `demo/small.py`, and the output shows the clustering performance of [mouse bladder cell](https://github.com/ttgump/scDeepCluster/tree/master/scRNA-seq%20data):

```
Pretrain begin============================================
Step 50(5.0%): Batch Loss=494.993896484375
Step 100(10.0%): Batch Loss=473.5043029785156
...
Step 950(95.0%): Batch Loss=432.5137634277344
Step 1000(100.0%): Batch Loss=412.89599609375
Pretrain end.============================================
Step 50(0.25%); Global Step 50: Batch Loss=584.4363403320312; [Reconstruct, MDS, L2] Loss = [523.41284, 5.085291, 0.0]
...
Step 5100(25.5%); Global Step 5100: Validation Loss=468.5406188964844; [Reconstruct, MDS, L2] Loss = [451.28278, 1.4381549, 0.0]; Min Val Loss = 467.6856384277344; No Improve = 5; 
Step 5150(25.75%); Global Step 5150: Batch Loss=468.8702392578125; [Reconstruct, MDS, L2] Loss = [450.14258, 1.5606391, 0.0]
Step 5200(26.0%); Global Step 5200: Batch Loss=459.5696716308594; [Reconstruct, MDS, L2] Loss = [443.746, 1.3186402, 0.0]
No improve = 6, early stop!
Training end. Total step = 5200
Type of embedding = <class 'numpy.ndarray'>; Shape of embedding = (2746, 256); Data type of embedding = float32
RPH-KMeans (n_init = 1): ARI = 0.6105 (0.0279), NMI = 0.7604 (0.0086)
RPH-KMeans (n_init = 10): ARI = 0.6679 (0.0630), NMI = 0.7754 (0.0135)
KMeans (init = k-means++, n_init = 1): ARI = 0.5785 (0.0377), NMI = 0.7644 (0.0126)
KMeans (init = k-means++, n_init = 10): ARI = 0.5821 (0.0302), NMI = 0.7648 (0.0070)
```

The running config, history of loss and output log will be provided by **AIDE** in `aide_for_bladder` folder, and here shows the figure of history loss:

![history loss](demo/aide_for_bladder/loss.png)


### For Large Data (e.g. cell_num > 100000)
A demo is provided in `demo/large.py`, and the output shows the clustering performance of [Shekhar mouse retina](https://github.com/ttgump/scDeepCluster/tree/master/scRNA-seq%20data/large%20real%20datasets):

```
Pretrain begin============================================
Step 50(5.0%): Batch Loss=458.53656005859375
Step 100(10.0%): Batch Loss=443.490234375
...
Step 950(95.0%): Batch Loss=420.61492919921875
Step 1000(100.0%): Batch Loss=427.2901611328125
Pretrain end.============================================
Step 50(0.25%); Global Step 50: Batch Loss=469.0723876953125; [Reconstruct, MDS, L2] Loss = [445.16647, 1.9921587, 0.0]
...
Step 5700(28.5%); Global Step 5700: Validation Loss=436.14697265625; [Reconstruct, MDS, L2] Loss = [429.66595, 0.54008174, 0.0]; Min Val Loss = 435.47027587890625; No Improve = 5; 
Step 5750(28.75%); Global Step 5750: Batch Loss=434.73870849609375; [Reconstruct, MDS, L2] Loss = [428.11176, 0.5522464, 0.0]
Step 5800(29.0%); Global Step 5800: Batch Loss=439.7626037597656; [Reconstruct, MDS, L2] Loss = [432.93768, 0.56874436, 0.0]
No improve = 6, early stop!
Training end. Total step = 5800
Type of embedding = <class 'numpy.ndarray'>; Shape of embedding = (27499, 256); Data type of embedding = float32
RPH-KMeans (n_init = 1): ARI = 0.8914 (0.0306), NMI = 0.8248 (0.0117)
RPH-KMeans (n_init = 10): ARI = 0.8859 (0.0155), NMI = 0.8246 (0.0054)
KMeans (init = k-means++, n_init = 1): ARI = 0.7556 (0.1552), NMI = 0.7944 (0.0228)
KMeans (init = k-means++, n_init = 10): ARI = 0.6659 (0.1138), NMI = 0.7895 (0.0209)
```

The running config, history of loss and output log will be provided by **AIDE** in `aide_for_shekhar` folder, and here shows the figure of history loss:

![history loss](demo/aide_for_shekhar/loss.png)


## File Illustation
```
result
	data_explain_paper: basic infomation of data
	dim_origin_clt_eval_paper: output of dim_origin_clt_eval.py
	dim_reduce_eval_paper: output of dim_reduce_clt_eval.py
src
	preprocess.py: data preprocess
	dim_origin_clt_eval.py: data preprocess + clustering
	dim_reduce_clt_eval.py: data preprocess + dimension reduction + clustering
demo
	data: data of demo
	small.py: demo to handle small data
	large.py: demo to handle large data
data (not uploaded)
	raw
		1M_neurons
			1M_neurons_filtered_gene_bc_matrices_h5.h5
		sc_brain
			l5_all.loom
		scDeepCluster
			PBMC_68k
				PBMC_68k.h5
			Shekhar_mouse_retina
				Shekhar_mouse_retina.h5
			10X_PBMC
				10X_PBMC.h5
			mouse_bladder_cell
				mouse_bladder_cell.h5
			mouse_ES_cell
				mouse_ES_cell.h5
			worm_neuron_cell
				worm_neuron_cell.h5
``` 

